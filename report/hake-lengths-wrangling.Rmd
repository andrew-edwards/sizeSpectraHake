---
title: "Analyses of hake lengths using size spectra approach -- data wrangling of original and updated Dec 2023 data"
author: "Andrew Edwards"
output: pdf_document
fontsize: 12pt
date: "Last compiled on `r format(Sys.time(), '%d %B, %Y')`"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "",
  fig.width = 10,
  fig.height = 8
)
```

```{r, packages}
library(dplyr)
library(sizeSpectra)    # Probably need latest version
library(pacea)
# load_all()  # Don't need for the wrangling.
```

Data from the NOAA SWFSC Rockfish Recruitment and Ecosystem Assessment Survey
(RREAS). Original file from Iain and Hayden (originally from John Field), November 2023 version directly from
John Field.

Load in the original data set (saving objects as "...orig") and November 2023
version (saving objects as "...nov23") sent by John
Field. Wrangle them both the same way, then load into `analysis-orig-C.Rmd`
etc. files to do the same analyses on each.

Given that plotting results highlights issues with the data (such as some being
to nearest 10 mm not 1 mm -- see notes from John), may end up saving
various versions of the data and come back here to do additional wrangling.

Looking at `cpue-length_hake-hayden....csv` from
Iain (15th May 2023). Don't think have looked at `hake_lenclasses_cpue.csv` as downloaded from Google Drive owned by Ed
Weber, 10th May 2023.

Waiting for reply: For a length measured as 30 mm, I need to specify a corresponding length
   bin. Should this be 29.5-30.5 mm, or 30-31mm? I've assumed the former but can
   easily change it.

## Load in original raw data

Original from La Jolla trip:

```{r loadsetdata}
raw_orig <- readr::read_csv("../../size-spectra-applications/calcofi-hake/hake-lengths/cpue-length_hake-hayden's weight file with all fish in an annual super tow.csv") %>%
  dplyr::mutate_if(is.character, factor)

raw_orig

summary(raw_orig)
```

`CPUE_YOY` -- the CPUE of young-of-year hake caught in that haul. The rows
for that haul are only the fish that were measured (sampled), each having a
length of `STD_LENGTH`.

`WEIGHT` -- not using, think it just the length-weight relationship applied to
each `STD_LENGTH`.

## Load in updated nov23 raw data

Updated data (including 2022 and 2023) from John Field. Different format so want
to compare results with original data.

```{r loadnov23}
raw_nov23 <-
  readr::read_csv("../../size-spectra-applications/calcofi-hake/hake-lengths/rreas-to-2023/hake83to2023lengths.csv") %>%
  dplyr::mutate_if(is.character, factor)

raw_nov23

summary(raw_nov23)
```

Each row is a fish (given a `SP_NO`, specimen).

## Define bin widths for each year

Define bin widths for each year. Original data had 0.01 mm for 2021, which, as
John explained, is a bit preliminary and they then save it to 1 mm anyway; so
nov23 data, and presumably onwards, is all to 1mm. so had to make a tibble
encompassing all years of data for original data:
```{r binwidth}
bin_width_each_year_orig <-
  tibble::tibble(year = min(raw_orig$YEAR):max(raw_orig$YEAR)) %>%
  mutate(bin_width = ifelse(year %in% c(2021),
                            0.01,
                            1))

bin_width_each_year_nov23 <-
  tibble::tibble(year = min(raw_nov23$YEAR):max(raw_nov23$YEAR)) %>%
  mutate(bin_width = 1)
```

## Validate understanding of data structure

First validate some assumptions to check I understand the data. Just do 2004
original data first then all years.
```{r, validate_2004}
raw_2004_orig <- filter(raw_orig,
                   YEAR == 2004)

raw_2004_orig

stopifnot(length(unique(raw_2004_orig$CRUISE)) == 1)   # One CRUISE per year

# Check each year-station combination has the same HAUL_NO and CPUE_YOY:
# Both of these return a warning (I'd prefer an error) about returning more than
# one line per group. So HAUL_NO and CPUE_YOY not unique per year-station
# combination. Hayden used YEAR, STRATA, HAUL_NO so try that after.
raw_2004_summ_a_orig <- summarise(group_by(raw_2004_orig,
                                           YEAR,
                                           STATION),
                                  haul_no = unique(HAUL_NO))#,

raw_2004_summ_a_orig

raw_2004_summ_b_orig <- summarise(group_by(raw_2004_orig,
                                           YEAR,
                                           HAUL_NO),
                                  cpue_yoy = unique(CPUE_YOY))  # No error but
        # message only mentions grouped by YEAR, not HAUL_NO also. Same above also.
raw_2004_summ_b_orig
# Yet:
length(unique(raw_2004_summ_b_orig$HAUL_NO))   # also has 92 rows. So seems okay,
                                        # message was curious.

# So seems like multiple hauls at a station, but each HAUL_NO does indeed have a
# unique CPUE_YOY.

# Each SP_NO is unique, a code for each specimen (individual fish)
expect_equal(nrow(raw_2004_orig),
             length(unique(raw_2004_orig$SP_NO)))
```

That makes more sense. Repeat the essential from above for all data, grouped by year:

```{r validate_all}
# Doing year-by-year to see when errors (with a warning). Okay: 2004, 2005, ..., 2014, 2017,
#  2018, 2020, 2021
# Not okay: 2015, 2016, 2019
raw_summ_orig <- summarise(group_by(filter(raw_orig,
                                      YEAR == 2015),
                               YEAR,
                               HAUL_NO),
                      cpue_yoy = unique(CPUE_YOY))

raw_summ_orig

# So 2015 does not have a unique CPUE_YOY for each HAUL_NO. Aha - check STRATA
# as well (Hayden used in his unique ID for each haul:

# These work for 2015, 2016, 2019.
raw_XXXX_summ_orig <- summarise(group_by(filter(raw_orig,
                                                YEAR == 2019),
                                         YEAR,
                                         HAUL_NO,
                                         STRATA),  # Don't get why not grouped by
                                        # STRATA also
                                cpue_yoy = unique(CPUE_YOY))
```
So each `YEAR-HAUL_NO-STRATA` combination correctly has a unique `CPUE_YOY`;
hence Hayden used that to assign a `UniqueID`

Check `CPUE_YOY` are all integers (think they are because of one-hour tows; TODO read papers):
```{r, validatesp}
expect_equal(sort(unique(raw_orig$CPUE_YOY)) %>%
             diff() %>%
             min(),
             1)

```

Not going to use OR or WA so just keep the others.
```{r stratakeep}
strata_to_keep <- c("C", "NC", "S", "SC", "N")
```

## Simplify original data set

```{r, simplify}
raw_simp_orig <- filter(raw_orig,
                   STD_LENGTH >= 28,   # TODO check with Iain, he said 28 onwards
                                       # was > 27 but isn't actually as 0.01mm
                                       # resolution. Think okay now as nov23 has
                                       # 1 mm. Of course will get xmin=27.5.
                   STD_LENGTH < 90,
                   STRATA %in% strata_to_keep) %>%
  select(year = YEAR,
         haul_no = HAUL_NO,
         strata = STRATA,
         x = STD_LENGTH,
         cpue_yoy = CPUE_YOY) %>%
  mutate(id = paste(year,       # Hayden's id, to make calcs easier
                    strata,     # than grouping, and to more easily compare any results.
                    haul_no,
                    sep = "_"),
         haul_no = as.factor(haul_no),
         id = as.factor(id)
         ) %>%
  arrange(year,
          strata,
          haul_no)

raw_simp_orig

# One row for each haul:
raw_simp_totals_orig <- raw_simp_orig %>%
  group_by(id) %>%
  summarise(measured_in_haul = n())
hist(raw_simp_totals_orig$measured_in_haul, breaks = 40)
# Shows don't have enough data to fit at the haul level - kind of, have to check
# final 'counts' TODO down below, maybe.

# Calculate proportion of cpue_yoy that were measured, kind of assuming
#  cpue_yoy is an absolute count of fish, which it seems to be as it's always
#  integer. If effort is always the same for every tow, then maybe an effort of
#  1 is just a standard tow, hence there's no scaling (and hence cpue is an
#  integer; think it's one-hour tow). TODO read papers.

# Then scale the counts up by 1/prop_measured (i.e. if only half the fish in a
#  haul were measured, then double the count for each size). Easier to see the
#  consequences of this in results files, as some values get scaled up a lot.
raw_simp_prop_orig <- left_join(raw_simp_orig,
                                raw_simp_totals_orig,
                                by = "id") %>%
  mutate(prop_measured = measured_in_haul / cpue_yoy,
         scaled_counts = 1 / prop_measured)

raw_simp_prop_orig
summary(raw_simp_prop_orig)

# Lengths are not always integers - turns out in 2021 they are to nearest 0.01
# mm (and maybe later years if get more data).
sort(unique(raw_simp_prop_orig$x))[1:10]

stopifnot(max(raw_simp_orig$year) == 2021)   # just a check, but will call new
                                        # data something different anyway (and
                                        # based wrangling on nov23).
```

## New data set

Given had already figured out above details, just filter to what we ended up
needing in `raw_simp_prop_orig`, then compare fitted results (and come back to this
if needed).

```{r newdata}
# From above for original data:
# names(raw_simp_prop_orig)
# "year"             "haul_no"          "strata"           "x"    # "cpue_yoy"         "id"               "measured_in_haul" "prop_measured"
# "scaled_counts"

raw_simp_nov23 <- select(raw_nov23,
                       year = YEAR,
                       haul_no = HAUL_NO,
                       strata = STRATA,
                       x = STD_LENGTH,
                       total_no = TOTAL_NO,
                       number_measured = NMEAS, # TODO double check (if results
                                        # with orig don't match then look more)
                       exp_value = EXP) %>%
  filter(strata %in% strata_to_keep) %>%
  mutate(id = paste(year,       # Hayden's id, to make calcs easier
                    strata,     # than grouping, and to more easily compare any results.
                    haul_no,
                    sep = "_"),
         haul_no = as.factor(haul_no),
         id = as.factor(id)) %>%
  arrange(year,
          strata,
          haul_no)

raw_simp_nov23

# Think this is already calc'ed in new data; if results don't match with orig
# then may have to dig into.
# raw_simp_totals_nov23 <- raw_simp_nov23 %>%
#  group_by(id) %>%
#  summarise(measured_in_haul = n())

raw_simp_prop_nov23 <- mutate(raw_simp_nov23,
                            prop_measured = number_measured/total_no,
                            scaled_counts = 1 / prop_measured) %>%  # The 1 is 1 obs for each row
  filter(number_measured > 0)

expect_equal(raw_simp_prop_nov23$exp_value,
             raw_simp_prop_nov23$scaled_counts)

# Last line gives no error. Excellent, is what I thought. So can remove
#  exp_value (which was in the file).
raw_simp_prop_nov23 <- select(raw_simp_prop_nov23,
                              -"exp_value")

raw_simp_prop_nov23
summary(raw_simp_prop_nov23)

stopifnot(max(raw_simp_nov23$year) == 2023)   # if get new data then need to
                                              # double check and change
                                              # bin_width if needed. Likely all
                                              # 1 mm though.
```

Resolution, all integers:

```{r resolution}
length(unique(raw_simp_prop_nov23$x))
sort(unique(raw_simp_prop_nov23$x))
```
TODO probably best to make names exactly the same as `raw_simp_prop_orig`. Need
to get on with analysis though -- should make them consistent with nov23 rather
than orig.

Check what data are available for each year:
```{r nov23summary}
summary_nov23 <- summarise(group_by(raw_simp_prop_nov23,
                                  year,
                                  strata),
                         total_counts = sum(scaled_counts)) %>%
  ungroup()
summary_nov23 %>% as.data.frame()
```

### Check `number_measured` agrees with number of rows

`number_measured` should, for each `id`, be the number of rows for that `id`.

```{r checknumber}
check_number_measured <- summarise(group_by(raw_simp_prop_nov23,
                                            id),
                                   number_measured_sum = sum(number_measured),
                                   number_rows = n())
#                                   number_diff = number_measured - number_rows)
```

Gave a WARNING with `unique(number_measured)` instead of `sum()`. So the issue
is that `number_measured` is not unique to a haul, sometimes two lots are done.

NOT ACTUALLY THE sum, need to do in two steps. Current answer is actually the
square of `number_rows`.
So HERE HERE, next check `number_rows` is the sum of `number_measured` for each
id, then use `sum` not `unique` either earlier or later to save data. Want to
have one row for each `id`, and don't currently have that.

CAN PROB DELETE THIS - maybe commit first then delete, so not completely lost.
which I think means `number_measured` is indeed not
unique. Should dig into to get rid of warning, though not easy. Maybe need to do
by each year separately. Aha, `mtcars` example in `?dplyr` does give rows with
the grouping repeated, so here I should have multiple rows with the same `id`.

```{r checknumber2}
id_repeated_ind <- table(check_number_measured$id)[which(table(check_number_measured$id) > 1)]
id_repeated <- filter(check_number_measured, id %in% names(id_repeated_ind))
id_repeated
```
Aha, but they look like they add up

```{r checknumber3}
id_repeated_grouped <- summarise(group_by(id_repeated,
                                          id),
                                   number_measured = unique(number_measured),
                                   number_rows = n())

```


## Save two wrangled data files and bin widths

Best not to save them as package objects as will get confusing when not committing them.

```{r savedata}
saveRDS(raw_simp_prop_orig, "raw_simp_prop_orig.rds")
saveRDS(raw_simp_prop_nov23, "raw_simp_prop_nov23.rds")

saveRDS(bin_width_each_year_orig, "bin_width_each_year_orig.rds")
saveRDS(bin_width_each_year_nov23, "bin_width_each_year_nov23.rds")
```

## Some ideas to keep here for reference

Some original ideas I think:

Do lengths from 28 to 89 mm.

Whole coast do 2004 onwards, for just "C" can do all years (i.e. back to
1994). TODO check the regions though excplicitly. TODO - in `summary_nov23` below
it looks like 2001 is okay also.

N only from 2013 onwards, so not much.

Combine N and NC, though early years not sampled so could just do NC.

Do NC, C, SC, S, from 2004 onwards. Separately and combined, though more
southern don't have such a range.

Make sure to not use OR and WA. WA is only 2011 and 2016. WA is 2011 2014 2015 2016 2017 2019.
<!-- unique(filter(raw, STRATA == "OR")$YEAR) -->

Try 2 mm bins for coastwide, Iain thinks should should affect the cutting off issue for 2014.

D Ignore WA and OR (only 1 cm resolution), different surveys.

Convert to body mass first, check results come out the same (Charlie). Though
lengths are working fine, probably best to stick with these.

Larvae. See `hake-lengths-larvae.Rmd` for what I started quickly on final day at
La Jolla.

D Rename columns for simplicity.

TODO functionalise with functions early on here, may want to do in a new package.

Last (commented code) shows that only 2021 has lengths to 0.01 mm. So deal with that
separately below.

$b_l$ to $b_w$ figures -- see code at end of `hake-lengths-analysis.Rmd`.
