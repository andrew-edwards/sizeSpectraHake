---
title: "Analyses of hake lengths using size spectra approach -- data wrangling of original and updated Dec 2023 data"
author: "Andrew Edwards"
output: pdf_document
fontsize: 12pt
date: "Last compiled on `r format(Sys.time(), '%d %B, %Y')`"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "",
  fig.width = 10,
  fig.height = 8
)
```

```{r, packages}
library(dplyr)
library(sizeSpectra)    # Probably need latest version
library(pacea)
# load_all()  # Don't need for the wrangling.
```

Data from the NOAA SWFSC Rockfish Recruitment and Ecosystem Assessment Survey
(RREAS). Original file from Iain and Hayden (originally from John Field), November 2023 version directly from
John Field.

Load in the original data set (saving objects as "...orig") and November 2023
version (saving objects as "...nov23") sent by John
Field. Wrangle them both the same way, then load into `analysis-orig-C.Rmd`
etc. files to do the same analyses on each.

Given that plotting results highlights issues with the data (such as some being
to nearest 10 mm not 1 mm -- see notes from John), may end up saving
various versions of the data and come back here to do additional wrangling.

Looking at `cpue-length_hake-hayden....csv` from
Iain (15th May 2023). Don't think have looked at `hake_lenclasses_cpue.csv` as downloaded from Google Drive owned by Ed
Weber, 10th May 2023.

Waiting for reply: For a length measured as 30 mm, I need to specify a corresponding length
   bin. Should this be 29.5-30.5 mm, or 30-31mm? I've assumed the former but can
   easily change it.

## Load in original raw data

Original from La Jolla trip:

```{r loadsetdata}
raw_orig <- readr::read_csv("../../size-spectra-applications/calcofi-hake/hake-lengths/cpue-length_hake-hayden's weight file with all fish in an annual super tow.csv") %>%
  dplyr::mutate_if(is.character, factor)

raw_orig

summary(raw_orig)
```

`CPUE_YOY` -- the CPUE of young-of-year hake caught in that haul. The rows
for that haul are only the fish that were measured (sampled), each having a
length of `STD_LENGTH`.

`WEIGHT` -- not using, think it just the length-weight relationship applied to
each `STD_LENGTH`.

## Load in updated nov23 raw data

Updated data (including 2022 and 2023) from John Field. Different format so want
to compare results with original data.

```{r loadnov23}
raw_nov23 <-
  readr::read_csv("../../size-spectra-applications/calcofi-hake/hake-lengths/rreas-to-2023/hake83to2023lengths.csv") %>%
  dplyr::mutate_if(is.character, factor)

raw_nov23

summary(raw_nov23)
```

Each row is a fish (given a `SP_NO`, specimen).

## Define bin widths for each year

Define bin widths for each year. Original data had 0.01 mm for 2021, which, as
John explained, is a bit preliminary and they then save it to 1 mm anyway; so
nov23 data, and presumably onwards, is all to 1mm. so had to make a tibble
encompassing all years of data for original data:
```{r binwidth}
bin_width_each_year_orig <-
  tibble::tibble(year = min(raw_orig$YEAR):max(raw_orig$YEAR)) %>%
  mutate(bin_width = ifelse(year %in% c(2021),
                            0.01,
                            1))

bin_width_each_year_nov23 <-
  tibble::tibble(year = min(raw_nov23$YEAR):max(raw_nov23$YEAR)) %>%
  mutate(bin_width = 1)
```

## Validate understanding of data structure

First validate some assumptions to check I understand the data. Just do 2004
original data first then all years.
```{r, validate_2004}
raw_2004_orig <- filter(raw_orig,
                   YEAR == 2004)

raw_2004_orig

stopifnot(length(unique(raw_2004_orig$CRUISE)) == 1)   # One CRUISE per year

# Check each year-station combination has the same HAUL_NO and CPUE_YOY:
# Both of these return a warning (I'd prefer an error) about returning more than
# one line per group. So HAUL_NO and CPUE_YOY not unique per year-station
# combination. Hayden used YEAR, STRATA, HAUL_NO so try that after.
raw_2004_summ_a_orig <- summarise(group_by(raw_2004_orig,
                                           YEAR,
                                           STATION),
                                  haul_no = unique(HAUL_NO))#,

raw_2004_summ_a_orig

raw_2004_summ_b_orig <- summarise(group_by(raw_2004_orig,
                                           YEAR,
                                           HAUL_NO),
                                  cpue_yoy = unique(CPUE_YOY))  # No error but
        # message only mentions grouped by YEAR, not HAUL_NO also. Same above also.
raw_2004_summ_b_orig
# Yet:
length(unique(raw_2004_summ_b_orig$HAUL_NO))   # also has 92 rows. So seems okay,
                                        # message was curious.

# So seems like multiple hauls at a station, but each HAUL_NO does indeed have a
# unique CPUE_YOY.

# Each SP_NO is unique, a code for each specimen (individual fish)
expect_equal(nrow(raw_2004_orig),
             length(unique(raw_2004_orig$SP_NO)))
```

That makes more sense. Repeat the essential from above for all data, grouped by year:

```{r validate_all}
# Doing year-by-year to see when errors (with a warning). Okay: 2004, 2005, ..., 2014, 2017,
#  2018, 2020, 2021
# Not okay: 2015, 2016, 2019
raw_summ_orig <- summarise(group_by(filter(raw_orig,
                                      YEAR == 2015),
                               YEAR,
                               HAUL_NO),
                      cpue_yoy = unique(CPUE_YOY))

raw_summ_orig

# So 2015 does not have a unique CPUE_YOY for each HAUL_NO. Aha - check STRATA
# as well (Hayden used in his unique ID for each haul:

# These work for 2015, 2016, 2019.
raw_XXXX_summ_orig <- summarise(group_by(filter(raw_orig,
                                                YEAR == 2019),
                                         YEAR,
                                         HAUL_NO,
                                         STRATA),  # Don't get why not grouped by
                                        # STRATA also
                                cpue_yoy = unique(CPUE_YOY))
```
So each `YEAR-HAUL_NO-STRATA` combination correctly has a unique `CPUE_YOY`;
hence Hayden used that to assign a `UniqueID`

Check `CPUE_YOY` are all integers (think they are because of one-hour tows; TODO read papers):
```{r, validatesp}
expect_equal(sort(unique(raw_orig$CPUE_YOY)) %>%
             diff() %>%
             min(),
             1)

```

Not going to use OR or WA so just keep the others.
```{r stratakeep}
strata_to_keep <- c("C", "NC", "S", "SC", "N")
```

## Simplify original data set

```{r, simplify}
raw_simp_orig <- filter(raw_orig,
                   STD_LENGTH >= 28,   # TODO check with Iain, he said 28 onwards
                                       # was > 27 but isn't actually as 0.01mm
                                       # resolution. Think okay now as nov23 has
                                       # 1 mm. Of course will get xmin=27.5.
                   STD_LENGTH < 90,
                   STRATA %in% strata_to_keep) %>%
  select(year = YEAR,
         haul_no = HAUL_NO,
         strata = STRATA,
         x = STD_LENGTH,
         cpue_yoy = CPUE_YOY) %>%
  mutate(id = paste(year,       # Hayden's id, to make calcs easier
                    strata,     # than grouping, and to more easily compare any results.
                    haul_no,
                    sep = "_"),
         haul_no = as.factor(haul_no),
         id = as.factor(id)
         ) %>%
  arrange(year,
          strata,
          haul_no)

raw_simp_orig

# One row for each haul:
raw_simp_totals_orig <- raw_simp_orig %>%
  group_by(id) %>%
  summarise(measured_in_haul = n())
hist(raw_simp_totals_orig$measured_in_haul, breaks = 40)
# Shows don't have enough data to fit at the haul level - kind of, have to check
# final 'counts' TODO down below, maybe.

# Calculate proportion of cpue_yoy that were measured, kind of assuming
#  cpue_yoy is an absolute count of fish, which it seems to be as it's always
#  integer. If effort is always the same for every tow, then maybe an effort of
#  1 is just a standard tow, hence there's no scaling (and hence cpue is an
#  integer; think it's one-hour tow). TODO read papers.

# Then scale the counts up by 1/prop_measured (i.e. if only half the fish in a
#  haul were measured, then double the count for each size). Easier to see the
#  consequences of this in results files, as some values get scaled up a lot.
raw_simp_prop_orig <- left_join(raw_simp_orig,
                                raw_simp_totals_orig,
                                by = "id") %>%
  mutate(prop_measured = measured_in_haul / cpue_yoy,
         scaled_counts = 1 / prop_measured)

raw_simp_prop_orig
summary(raw_simp_prop_orig)

# Lengths are not always integers - turns out in 2021 they are to nearest 0.01
# mm (and maybe later years if get more data).
sort(unique(raw_simp_prop_orig$x))[1:10]

stopifnot(max(raw_simp_orig$year) == 2021)   # just a check, but will call new
                                        # data something different anyway (and
                                        # based wrangling on nov23).
```

## New data set

Given had already figured out above details, just filter to what we ended up
needing in `raw_simp_prop_orig`, then compare fitted results (and come back to this
if needed).

```{r newdata}
# From above for original data:
# names(raw_simp_prop_orig)
# "year"             "haul_no"          "strata"           "x"    # "cpue_yoy"         "id"               "measured_in_haul" "prop_measured"
# "scaled_counts"

raw_simp_nov23 <- select(raw_nov23,
                       year = YEAR,
                       haul_no = HAUL_NO,
                       strata = STRATA,
                       x = STD_LENGTH,
                       total_no = TOTAL_NO,
                       number_measured = NMEAS, # TODO double check (if results
                                        # with orig don't match then look more)
                       exp_value = EXP) %>%
  filter(strata %in% strata_to_keep) %>%
  mutate(id = paste(year,       # Hayden's id, to make calcs easier
                    strata,     # than grouping, and to more easily compare any results.
                    haul_no,
                    sep = "_"),
         haul_no = as.factor(haul_no),
         id = as.factor(id)) %>%
  arrange(year,
          strata,
          haul_no)

raw_simp_nov23

# Think this is already calc'ed in new data; if results don't match with orig
# then may have to dig into.
# raw_simp_totals_nov23 <- raw_simp_nov23 %>%
#  group_by(id) %>%
#  summarise(measured_in_haul = n())

raw_simp_prop_nov23 <- mutate(raw_simp_nov23,
                            prop_measured = number_measured/total_no,
                            scaled_counts = 1 / prop_measured) %>%  # The 1 is 1 obs for each row
  filter(number_measured > 0)

expect_equal(raw_simp_prop_nov23$exp_value,
             raw_simp_prop_nov23$scaled_counts)

# Last line gives no error. Excellent, is what I thought. So can remove
#  exp_value (which was in the file).
raw_simp_prop_nov23 <- select(raw_simp_prop_nov23,
                              -"exp_value")

raw_simp_prop_nov23
summary(raw_simp_prop_nov23)

stopifnot(max(raw_simp_nov23$year) == 2023)   # if get new data then need to
                                              # double check and change
                                              # bin_width if needed. Likely all
                                              # 1 mm though.
```

Resolution, all integers:

```{r resolution}
length(unique(raw_simp_prop_nov23$x))
sort(unique(raw_simp_prop_nov23$x))
```
Had thought probably best to make names exactly the same as `raw_simp_prop_orig`. Need
to get on with analysis though, and if want to redo orig then make that data
consistent with nov23.

Check what data are available for each year, though `total_counts` can be a bit
misleading as it's scaled up; `total_number_measured` is the number of rows so
will be the total number of fish measured, whereas `total_counts` is scaled up
based on proportion of what was caught that were measured.
```{r nov23summary}
summary_nov23 <- summarise(group_by(raw_simp_prop_nov23,
                                  year,
                                  strata),
                           total_number_measured = n(),
                           total_counts = sum(scaled_counts)) %>%
  ungroup()
summary_nov23 %>% as.data.frame()
```

### Understand when `number_measured` does not agree with number of rows

Had originally assumed that `number_measured` should, for each `id`, be the number of rows for that
`id`. But some `id` have more than two `number_measured`. Then wondered if they
correspond to 1mm and 10mm measurements (i.e. different sampling
protocols). Answer is usually yes, but in 2022 and 2023 there are sometimes two
`sets` of measurements, as figured out below.

So first see when
`number_measured` does not add up to the number of rows.
First tried `summarise` here but gives a warning because `unique(number_measured)` is not
unique, so using the suggested `reframe` instead:

```{r checknumber}
check_number_measured <- reframe(group_by(raw_simp_prop_nov23,
                                            id),
                                   number_measured = unique(number_measured),
                                   number_rows = n())
```

So this allows two rows for some ids. These ids are:
```{r idstworows}
id_repeated_ind <-
  table(check_number_measured$id)[which(table(check_number_measured$id) > 1)]
id_repeated <- filter(check_number_measured,
                      id %in% names(id_repeated_ind))
id_repeated
```

(Note that later we find that `2022_C_5` has two sets but with the same
`total_no`, so need to deal with that early on to distinguish sets.

HERE HERE HERE - work out where to do the fix to deal with `2022_C_5`.

Also do `summarise` on `check_number_measured` [could also have done on
`id_repeated`] to group them properly to check the number of rows
```{r checknumber2}
check_number_measured_unique <- summarise(group_by(check_number_measured,
                                                   id),
                                          number_measured_sum =
                                            sum(number_measured),
                                          number_rows = unique(number_rows))
check_number_measured_unique
```
Correctly gives no warning. Now see that the two columns don't quite match:
```{r checknumber3}
ind_not_same <-
  which(check_number_measured_unique$number_measured_sum - check_number_measured_unique$number_rows != 0)
ind_not_same
not_same <- check_number_measured_unique[ind_not_same, ]
not_same
```

So look at those four in detail:
```{r checknumber4}
filter(raw_simp_prop_nov23,
       id %in% not_same$id) %>%
  select(-c("scaled_counts")) %>%   # to fit on page
  a()
```
Can see there are two groups of samples for each `id` here, going to call these 'sets'.

`2009_C_82` measured 2 in each set, but had caught 3 then 2 total, hence `prop_measured` is different. And
second of those are both 30-mm measurements (so I wondered if 10-mm resolution,
but think just a coincidence).

`2022_C_8` first catches 71, measured 20 of them, to 1mm resoultion, then caught
80 and measured 20, well, still to 1-mm resolution! Oh well. So just two sets of
samples that need combining.

`2023_C_7` first caught 100, measured 20 to 1-mm resolution. Then caught 178,
also measured 20 to 1-mm resolution.

`2023_C_27` caught 2254 fish, measured 20 to 1-mm resolution. Then caught 147
and measured 20 to 1-mm resolution.

TODO Figure out how to store these going forward.

As for the other ones in `id_repeated`, look at actual data, can look at the
unique lengths measured for each `id` and `total_no` combination, having to assume
(as implicitly done earlier I think) that there aren't two samples for the same
`id` that happen to have the same `total_no`.
```{r checknumber5}
id_repeated_full <- filter(raw_simp_prop_nov23,
                           id %in% id_repeated$id)
id_repeated_full_unique <- reframe(group_by(id_repeated_full,
                                            id,
                                            total_no),
                                   x = unique(x))
id_repeated_full_unique %>% a()

# summarise there gives the warning, as expected. So using reframe.
```
Manually looking through first 180 lines or so when printing as a data frame we
see the expected pattern: one set in each `id` looks indeed to be 1-mm resolution, and one set is
10-mm. Though can see that the final ones in 2023 just had two spearate
samples with 1-mm resolution.

So there are two sets for all these. Sometimes due to resolution, sometimes
not.

So clearly have to check properly, and add a
resolution column. Could have a set A and a set B for these to also distinguish
them, though the `total_no` does distinguish them.

Do it here, then do for all `id`s.
```{r idsmm}
# Adapting from analysis-nov23-C.Rmd but hadn't realised the multiple rows for
# same strata issue then. Better to deal with all this here.

# Vector of measured lengths that aren't multiples of 10 mm
non_10_mm_measurements <-
  unique(id_repeated_full_unique$x)[unique(id_repeated_full_unique$x) %% 10 > 0] %>%
  sort()

non_10_mm_measurements

# Actually have to do this differently to other file, since need 1mm and 10mm.

# TODO RENAME when figured out
id_repeated_full_unique_non_10_mm <- mutate(id_repeated_full_unique,
                                        non_10_mm = (x %in% non_10_mm_measurements))
                                 # non_10_mm - is this

id_repeated_full_unique_non_10_mm
# summarise correctly gives error with unique. So use reframe then look for
#  ids-total_no combinations that have only non_10_mm = FALSE.
id_total_no_resolution <- summarise(group_by(id_repeated_full_unique_non_10_mm,
                                       id,
                                       total_no),
                                    "1_mm_res" = any(non_10_mm)) %>%   # at least 1 is TRUE, i.e. at least one 1-mm measurement
  ungroup()
id_total_no_resolution %>% a()
```
That says which `id` and `total_no` combinations have 1-mm resolution. Manually
went through can see get a `TRUE` and `FALSE` for each `id` from 2001-2009, then
2022 and 2023 there are only 1-mm resolution, but done in two sets.

Hmmm.... how did `2022_C_5` end up with just one row. Maybe that has 21 as a
repeated `total_no`. First check if any others with that.
```{r checknumber77}
summarise(group_by(id_total_no_resolution,
                   id),
          number = n()) %>%
  filter(number < 2)
```
So only that one. Look at original data:
```{r checknumber78}
filter(raw_simp_nov23,
       id == "2022_C_5") %>%
  a()
```
So `total_number` was the same for two sets, but can see that `number_measured`
wasn't (if it was then we would just see, say, `number_measured = 14` here but
28 rows; we correctly have 34 rows, the sum of the two `number_measured`. So
this means we should go back and group by `number_measured` also. So maybe
should give a `set` column.




```{r temp}
# HERE HERE - this does not quite work:

## reframe(group_by(id_repeated_full_unique_group,
##                  id,
##                  total_no),
##         all_10_mm = unique(non_10_mm)
##         )


## Rest of copying from analysis-nov23-C.Rmd, may be useful as template
# # ids which do not have non-10-mm measurements; i.e. only have 10 mm ones (the
# # 1-mm ids can still have some 10-mm measurements)
## ids_with_mm_accuracy <- filter(id_repeated_full_unique,
##                                x %in% non_10_mm_measurements)$id %>%
##                         unique()

## raw_with_10_mm_only <- filter(raw_simp_prop_strata,
##                               !(id %in% ids_with_mm_accuracy))
## raw_with_10_mm_only

## summary(raw_with_10_mm_only)

## ids_with_10_mm_only <- unique(raw_with_10_mm_only$id)

## ids_with_10_mm_only
```















---
Gave a WARNING with `unique(number_measured)` instead of `sum()`. So the issue
is that `number_measured` is not unique to a haul, sometimes two lots are done.

NOT ACTUALLY THE sum, need to do in two steps. Current answer is actually the
square of `number_rows`.
So HERE HERE, next check `number_rows` is the sum of `number_measured` for each
id, then use `sum` not `unique` either earlier or later to save data. Want to
have one row for each `id`, and don't currently have that. OR:

No - just realised that the reason there are two for some hauls might be that
some are 1mm and some 10mm. So check that.



CAN PROB DELETE THIS - maybe commit first then delete, so not completely lost.
which I think means `number_measured` is indeed not
unique. Should dig into to get rid of warning, though not easy. Maybe need to do
by each year separately. Aha, `mtcars` example in `?dplyr` does give rows with
the grouping repeated, so here I should have multiple rows with the same `id`.

Aha, but they look like they add up

```{r checknumber9}
id_repeated_grouped <- summarise(group_by(id_repeated,
                                          id),
                                   number_measured = unique(number_measured),
                                   number_rows = n())

```


## Save two wrangled data files and bin widths

Best not to save them as package objects as will get confusing when not committing them.

```{r savedata}
saveRDS(raw_simp_prop_orig, "raw_simp_prop_orig.rds")
saveRDS(raw_simp_prop_nov23, "raw_simp_prop_nov23.rds")

saveRDS(bin_width_each_year_orig, "bin_width_each_year_orig.rds")
saveRDS(bin_width_each_year_nov23, "bin_width_each_year_nov23.rds")
```

## Some ideas to keep here for reference

Some original ideas I think:

Do lengths from 28 to 89 mm.

Whole coast do 2004 onwards, for just "C" can do all years (i.e. back to
1994). TODO check the regions though excplicitly. TODO - in `summary_nov23` below
it looks like 2001 is okay also.

N only from 2013 onwards, so not much.

Combine N and NC, though early years not sampled so could just do NC.

Do NC, C, SC, S, from 2004 onwards. Separately and combined, though more
southern don't have such a range.

Make sure to not use OR and WA. WA is only 2011 and 2016. WA is 2011 2014 2015 2016 2017 2019.
<!-- unique(filter(raw, STRATA == "OR")$YEAR) -->

Try 2 mm bins for coastwide, Iain thinks should should affect the cutting off issue for 2014.

D Ignore WA and OR (only 1 cm resolution), different surveys.

Convert to body mass first, check results come out the same (Charlie). Though
lengths are working fine, probably best to stick with these.

Larvae. See `hake-lengths-larvae.Rmd` for what I started quickly on final day at
La Jolla.

D Rename columns for simplicity.

TODO functionalise with functions early on here, may want to do in a new package.

Last (commented code) shows that only 2021 has lengths to 0.01 mm. So deal with that
separately below.

$b_l$ to $b_w$ figures -- see code at end of `hake-lengths-analysis.Rmd`.
